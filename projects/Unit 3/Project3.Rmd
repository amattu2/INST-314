---
title: "Unit Project 3 Project"
authors: 201Team1, Yiling Du, Bryce Hawkins, Onyekachi Igwenagu, Alec Mattu, Delmar
  Randolph
date: "05/09/2021"
output:
  html_document:
    df_print: paged
---
**To:** Dr. Random

**From:** 201Team1, Yiling Du, Bryce Hawkins, Onyekachi Igwenagu, Alec Mattu, Delmar Randolph

**Subject:** Analysis of 110 automotive vehicle models from 2015


## Background
Our 2015 Vehicle Model Year Dataset is derived from data provided and compiled by KBB.com in 2015. Within this detailed analysis of our dataset, we want to derive inferences on various levels of means and proportions. Our population of interest revolves around vehicle models available in the United States during the 2015 new model release cycle (2015 model year). A snapshot of our dataset features and attributes is included below, with example values:

### Table Code
```{md}
|  Variable | Name|Example|Type (Quant. / Cat.)|
|:---------|:------------------------------------------|:----------------------------:|:-|
|    Make   | Vehicle manufacturer                              | Toyota                       |Categorical|
|   Model   | Vehicle model                                     | Prius                        |Categorical|
|    Type   | Vehicle category                                  | Hatchback |Categorical|
|  LowPrice | Lowest MSRP Available (In 1000s)                  | 6.5                          |Quant.|
| HighPrice | Highest MSRP Available (In 1000s)                 | 9.5                          |Quant.|
| Drive     | Vehicle output driveaxle                          | FWD                          |Categorical|
| CityMPG   | City miles per gallon avg                         | 36                           |Quant.|
| HwyMPG    | Highway miles per gallon avg                      | 44                           |Quant.|
| FuelCap   | Gas tank capacity in gallons                      | 12.5                         |Quant.|
| Length    | Bumper-to-bumper vehicle length (Inch)            | 178.5                        |Quant.|
| Width     | Door-to-door vehicle width (Inch)                 | 69.2                         |Quant.|
| Height    | Vehicle height from ground-to-roof (Inch)         | 58.1                         |Quant.|
| Wheelbase | Vehicle wheelbase (Inch)                          | 106.5                        |Quant.|
| UTurn     | Vehicle 180deg U-turn radius (Feet)               | 16                           |Quant.|
| Weight    | Vehicle dry weight (Lbs)                          | 2200                         |Quant.|
| Acc030    | Vehicle accel. to 30mph (Seconds)                 | 62                           |Quant.|
| Acc060    | Vehicle accel. to 60mph (Seconds)                 | 190                          |Quant.|
| QtrMile   | Vehicle time in seconds to 1/4th mile             | 92                           |Quant.|
| Size      | Vehicle size (Small, Midsized, Large)             | Large                        |Categorical|
```

### Table of Variables
|  Variable |Name|Example|Type (Quant. / Cat.)|
|:---------|:----------------------------|:---------------:|:-|
|    Make   | Vehicle manufacturer                              | Toyota                       |Categorical|
|   Model   | Vehicle model                                     | Prius                        |Categorical|
|    Type   | Vehicle category                                  | Hatchback |Categorical|
|  LowPrice | Lowest MSRP Available (In 1000s)                  | 6.5                          |Quant.|
| HighPrice | Highest MSRP Available (In 1000s)                 | 9.5                          |Quant.|
| Drive     | Vehicle output driveaxle                          | FWD                          |Categorical|
| CityMPG   | City miles per gallon avg                         | 36                           |Quant.|
| HwyMPG    | Highway miles per gallon avg                      | 44                           |Quant.|
| FuelCap   | Gas tank capacity in gallons                      | 12.5                         |Quant.|
| Length    | Bumper-to-bumper vehicle length (Inch)            | 178.5                        |Quant.|
| Width     | Door-to-door vehicle width (Inch)                 | 69.2                         |Quant.|
| Height    | Vehicle height from ground-to-roof (Inch)         | 58.1                         |Quant.|
| Wheelbase | Vehicle wheelbase (Inch)                          | 106.5                        |Quant.|
| UTurn     | Vehicle 180deg U-turn radius (Feet)               | 16                           |Quant.|
| Weight    | Vehicle dry weight (Lbs)                          | 2200                         |Quant.|
| Acc030    | Vehicle accel. to 30mph (Seconds)                 | 62                           |Quant.|
| Acc060    | Vehicle accel. to 60mph (Seconds)                 | 190                          |Quant.|
| QtrMile   | Vehicle time in seconds to 1/4th mile             | 92                           |Quant.|
| Size      | Vehicle size (Small, Midsized, Large)             | Large                        |Categorical|


## Analyses
```{r}
# Fixes a HTML compilation bug (ignore)
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
```

### Analysis 1: Chi-square Test of Independence
#### Analyst: Yiling Du


Topic for Analysis 1: test for the association between "Drive" variable and "Size" variable. Drive: the type of drive (FWD, RWD,AWD); Size: the size of car (Small, Midsized, Large). Each variable has three categories.


First, call the packages that are needed for the analysis:
```{r}
library(tidyverse)
#install.packages("sjstats")
library (sjstats)
library (pwr)
install.packages("car")
library (car)
```


Load Data:
```{r}
cars <- read.csv("./Cars2015.csv", header=T)
cars
str(cars)
```


View Data:
```{r}
dim(cars)
head(cars)
tail(cars)
glimpse(cars)
```


In this analysis, I'm going to look for an association between Drive and Size.
Drive should be categorical (factor), but is improperly assigned as a character variable.
Size should be categorical (factor), but is improperly assigned as a character variable.
I will change the data type in the below.

Show the code for variable extraction and clean-up:
First step:  extract the variables that I want to use and assign to a dataframe.
Second step: retain only observations with values for both variables
Third step: transform Drive to factor
Fourth step:  transform Size to factor
Fifth step:  view the results
```{r}
mydata<- cars %>% 
  select(Drive, Size)%>%
  filter(!is.na(Drive), !is.na(Size))
 glimpse(mydata)
 class(mydata$Drive)
 class(mydata$Size)
```
```{r}
mydata$Drive <- as.factor(mydata$Drive)
class(mydata$Drive)
levels(mydata$Drive)
```
```{r}
mydata$Size <- as.factor(mydata$Size)
class(mydata$Size)
levels(mydata$Size)
```
```{r}
head(mydata)
```


Create a table from the data (mydata):
```{r}
mytable<- table(mydata$Drive, mydata$Size)
mytable
```


Determine whether the data meets the conditions for performing a chi-square test of independence:
1: Independence, means each case that contributes a count to the table must be independent of all the other cases in the table. The data meets the independence condition.
2: Sample size, each particular cell must have at least 5 cases. The data meets the sample size condition.
3: degree of freedom, d.f should grater than 1. The data meets the d.f condition.


Apply the chisq test to the table:
```{r}
chisq.test(mytable)
```


Discussion/Interprettion:
1: explanatory variable: Drive (type of drive); response variable: Size (the size of a car).
2: Conditions:
a: Independence, means each case that contributes a count to the table must be independent of all the other cases in the table. The data meets the independence condition because each case is independent of other cases in the table.
b: Sample size, each particular cell must have at least 5 cases. Each cell have at least 5 cases in the contingency table, so the data meets the sample size condition.
c: degree of freedom, d.f should grater than 1. The data meets the d.f condition, d.f=(3-1)*(3-1)=4.
3: State Ho and Ha
Ho: The type of drive and the size of a car are independent (no association).
Ha: The type of drive and the size of a car are dependent (association).
4: X-squared = 4.1017, p-value = 0.3924, p-value > alpha=0.05, so fail to reject the null hypothesis.
5: There is no significant evidence to conclude that there is an association between the type of a drive and the size of the car.
6: Limitation: the data meets all the conditions for Chi-square Test of Independence, but the sample size (110) is still small comparing to all cars in the year of 2015, so the current sample size is not representative enough for all cars.




### Analysis 2: [Analysis of Variance]
#### Analyst:[Onyekachi Igwenagu]

Analysis 2 Topic: A test using the variables "Make and "FuelCap". The "Make" variable has upto two catagories: Acura, Chevrolet 

Now install the packages neeeded in order to complete our analysis. 

```{r}
library (dplyr)
library (sjstats)
library (car)
library (pwr)
```

Load in our data into R
```{r}
cars <- read.csv("./Cars2015.csv", header=T)
cars
```


Looking theought the the contents in the data. 
```{r}
mycat<- cars %>% 
select(FuelCap, Make)%>%
filter(!is.na(FuelCap), !is.na(Make))
head(cars)
attach(cars)
glimpse(cars)
dim(cars)
str(cars)
```


Im looking for the relationship between the variiables FuelCap and Make for in the data.Looking for a chaningin difference in the mean and standard deviation. The potentical catagorical explantory variable is Make and the quntitaive response variable is the FuelCap.  
```{r}
head(mycat)
names(mycat)
str(mycat)
summary(mycat)
sd(mycat$FuelCap)
tapply (mycat$FuelCap, mycat$Make, mean)
tapply (mycat$FuelCap, mycat$Make, sd)
tapply (mycat$FuelCap, mycat$Make, summary)
```

```{r}
group_by(cars, Make) %>%
summarise(
count = n(),
mean = mean(FuelCap, na.rm = TRUE),
sd = sd(FuelCap, na.rm = TRUE))
summary(mycat)
```

After summarizing the variables and looking at the graph, there looks to not be a  difference in the means of the sample size.. 

```{r}
qqnorm(mycat$FuelCap)
qqline(mycat$FuelCap)


```


```{r}
aov(mycat$FuelCap~mycat$Make, data = mycat)
anova.all = aov(mycat$FuelCap~mycat$Make, data = mycat)
summary(anova.all)

```


```{r}
TukeyHSD(anova.all)
```

```{r}
eta_sq(anova.all)
```

```{r}
pwr.anova.test(k=2, sig.level=0.05, f=1.321, n=10, power=NULL)

```


The explantory variable for this analysis is "Make" of the vehicels and the response variable is the FuelCap for each respective car Make. Before conduction the Analysis of Variance test the conditions where met. There was no significant difference in the mean for the Make. With the Test, we check the similarity of variance. The null is no difference in variances in all the groups involved. For the Hypothesis, Ho: muG_Make = muK_Make = muQ_Make, all the same Ha: mu≠mu, no difference can be seen so we do not reject the null. The p value from our summary calculations is 1.312. Power test is relevant because it allows you to determine the sample size required to detect an effect of a given size with a given degree of confidence level. There were not as many outliers in this data analysis all the outputs where all around the same level so there was not much change to the mean.  




### Analysis 3:Simple Linear Regression Between Two Quantitative Variables (CityMPG, Weight) 
#### Analyst:Bryce Hawkins

Before we begin, let's install the needed packages for the analysis:
```{r}
library(readr)
library(ggplot2)
```

Now that we have the packages installed, we will load in the Cars2015 dataset under "cars" and 
```{r}
cars <- read.csv("./Cars2015.csv", header=T)
```

Since Simple Linear Regression deals with two quantitative variables, we will observe both the Weight and CityMPG variables as seen below:
```{r}
cars$Weight
cars$CityMPG
```

Let's visualize the relationship between the two quantitative variables in the form of a scatterplot:
```{r}
ggplot(data = cars) +
   geom_point((mapping = aes(x=Weight, y=CityMPG, color = "red")))
```
Up above, from an eyeball-test we can see that the correlation looks to be strong and negative.

Now we will use the "cor" command to assess the correlation:
```{r}
cor(cars$CityMPG, cars$Weight)
```
From this assessment, we see that the correlation is indeed strong and negative, as -0.82 is close to -1, a perfect negative correlation.

To see if the data above meet the conditions for a linear model, we need to know what conditions need to be satisfied: 1) Linearity 2) Independent Residuals 3) Normal Residuals 4) Equal/constant Variability. The following tests are diagnostics to see if the data above meet the conditions for a linear model:

Linearity: We need to see if the relationship between the Explanatory variable (Weight) and the Response variable (CityMPG) is linear, using the plot command:
```{r}
plot(reg.out$residuals ~ cars$Weight)
abline(h = 0, lty = 3)
```

As we can see from above, the relationship between the Explanatory and Response variables is not linear.

Independent Residuals: For the sake of this analysis, we will assume that car Weight and car CityMPG are independent from one another.

Normal Residuals: Next, we will check to see if the residuals in this data are nearly normal using a histogram:
```{r}
hist(reg.out$residuals)
```
From this histogram, it does not look like the data has normal residuals, as the histogram appears to tail off towards the right.

Equal/Constant Variability: For the last test, we will check to see if the variability of points around the least square line is roughly constant, using the "qqnorm" and "qqline" commands:
```{r}
qqnorm(reg.out$residuals)
qqline(reg.out$residuals)
```
From what we see about, this data does meet the Equal/Constant Variability condition, as the points are roughly near the least square line.


Now, we will run the linear regression model, using the "lm" command:
```{r}
reg.out <- lm(CityMPG ~ Weight, data = cars)
```

We will summarize the data from the linear regression model down below:
```{r}
summary (reg.out)
```
Slope Interpretation: As car Weight increases, car CityMPG decreases by -.0043335.
Intercept Interpretation: When X (car Weight) is equal to 0, the Y (car CityMPG) is 37.4486063.

Discussion/Interpretation:
1) The explanatory variable used in this dataset is car weight, while the response variable is CityMPG.
2) There are four conditions to meet for this test, which are Linearity, Independent Residuals, Normal Residuals, and Equal/Constant Variability. When talking about linearity, this condition is not met, as the relationship between the explanatory and response variable is not linear. The independence condition is met, as t6he data in the histogram does not appear to be normally distributed. The equal/constant variability does appear to be met, as data points are rougly near the least square line.
3)Null Hypothesis: There is not a statistically significant relationship between car weight and   car CityMPG in 2015.
  Alternative Hypothesis: There is a statistically significant relationship between car weight and car CityMPG in 2015.
4) The linear regression model correlation can be described as a strong and negative              relationship.
  Slope Interpretation: As car weight increases, car CityMPG decreases by -0.0043335.
  Intercept Interpretation: When X (car weight) is equal to 0, the Y (car CityMPG) is 37.4486063.
5) Test Statistic: 0.05
   P-value: < 2.2e-16
   We fail to reject the null hypothesis, as the p-value is significantly lower than the test     statistic.
   R^2 Interpretation: Weight, as a predictor for CityMPG, accounts for 0.6838% of the            variability we see in CityMPG.
6) Conclusion: The variables in this dataset fail to meet all conditions for a linear model,      and we can conclude that there is a statistically significant relationship between car         weight and car CityMPG in 2015.
7) Limitations: All conditions were failed to be met, and there were outliers in this simple      linear regression.

### Analysis 4: MLR (Two quantitative variables, 1 response variable)
#### Analyst: Alec Mattu

##### Preface, Setup
Preface: We need to reference the appropriate packages.
```{r}
library(tidyverse)
library(car)
```

Let's begin the analysis by importing the Cars 2015 (Model year subset) dataset and cleaning rows with missing values.
```{r}
# Load Data
cars <- read.csv("./Cars2015.csv")
```

As a quick recap of the data, let's take a look at the various features in the dataset.
```{r}
# View Dimensions of the data
dim(cars)

# View column names
names(cars)
str(cars)

# View first few rows of data
head(cars)

# View last few rows of data
tail(cars)
```

##### Hypothesis/Summary
In this modern world that we live in, where fuel efficiency technology and manufacturing technology is constantly improving, what can we derive from analyzing the EPA rated City mpg on vehicles? I suspect that we can draw conclusions on what type of fuel tank capacity is required or how heavy a particular vehicle is based just off of the EPA rated fuel efficiency. 

$H_0$: There is a strong correlation between the city miles-per-gallon EPA rating on a vehicle and the fuel tank capacity (gal) as well as the vehicle dry-weight (lbs). I propose that we will see a negatively correlated linear relationship between the City MPG and the Fuel Capacity / Weight of a vehicle.

$H_A$: There is a insignificant or non-existent correlation between a vehicle's EPA rated City MPG and the fuel tank capacity / dry-weight.

##### Response Variable
The chosen response variable for this analysis will be `CityMPG`; To begin our actual analysis, let's visualize the data behind this variable.
```{r}
ggplot(data = cars) +
  xlab("EPA Rated City Mpg") +
  geom_boxplot(mapping = aes(x = CityMPG))
ggplot(data = cars) +
  xlab("EPA Rated City Mpg") +
  geom_histogram(mapping = aes(x = CityMPG), binwidth = 0.75)
```

From both of the above graphics, we can see that a majority of the observations in the dataset revolve around the ~20mpg area. There are a few concerning outliers above 30mpg, so let's dig deeper into those outliers before continuing. 

```{r}
cars_tibble <- as_tibble(cars[1:8])
cars_tibble %>% filter(CityMPG > 30)
```

From what we can see above, although these outliers originally appeared to be incorrect data, we can make the assumption that they are in fact correct. Not only can we fact check this ([2015 Mirage MPG](https://www.google.com/search?q=2015+mirage+mpg&oq=2015+mirage+mpg) | [2015 Versa Note MPG](https://www.google.com/search?q=2015+versa+note+mpg)), we can also base our assumption off of the fact that they are both relatively small hatchbacks and compare that to similar vehicles in that category.

##### Explanatory Variable 1
In order to find predictors of our response variable (`CityMPG`), let's use a correlation matrix. Although this was generalized previously, we need to find the finite list of columns which are numerical in order to predict the response variable. We're going to approach this model building from the "bottom up", where we find highly correlated variables and work from there.

```{r}
# Pull the complete columns from the dataset
numerical_columns <- cars[complete.cases(cars), ]
  
# Pull the complete numerical columns from the dataset
# Correlation matrix
complete_numerical_columns <- numerical_columns[, sapply(numerical_columns, is.numeric)]
print(round(cor(complete_numerical_columns), 2))

# Covariance matrix
print(round(cov(complete_numerical_columns), 2))
```

From the correlation display above, we can see quite a few correlations between `CityMPG` and other numerical variables. Let's start with the correlation between `CityMPG` and `FuelCap`. This is an interesting combination of variables because they are NEGATIVELY correlated with eachother. To be precise, they are correlated at `-0.77`--Which is logical, as `CityMPG` increases, the need for a higher gasoline capacity decreases (More fuel efficient). 

Let's analyze this relationship with a simple linear model.

```{r}
model1 <- lm(CityMPG ~ FuelCap, data = complete_numerical_columns)
summary(model1)
```

We can see from the above calculations that both our regular p-value and anova f-statistic P-value is extremely small `2e-16`, which is a good indication of a predictor variable. Additionally the $R^2$ value of `~0.60` (60%) isn't fantastic, but it isn't too weak to account for variability in the response variable.

Let's visualize the above correlations .

```{r}
ggplot(data = complete_numerical_columns, aes(x = CityMPG, y = FuelCap), na.rm = TRUE) +
  geom_point() +
  geom_smooth(method = 'lm')
```

Next let's run the standard residuals diagnostics on our first predictor variable.

```{r}
# Check for linearity
plot(model1$residuals ~ complete_numerical_columns$FuelCap)
abline(h = 0, lty = 3)

# Check for normality (1/2)
hist(model1$residuals)

# Check for normality (2/2)
qqnorm(model1$residuals)
qqline(model1$residuals)
```


From the above three diagrams, we can conclude:

a) The variable likely does not meet the linearity requirements; The response variable is not evenly scattered, and heavily condensed around the 15-20gal range (Linearity)

b) Based off of the histogram, we can see the distribution is relatively normal (Normality)

c) Similar to our linearity requirement, we don't neccesarily have equal variance as required. (Equal variance)

##### Explanatory Variable 2
Let's add the final predictor variable. The next highest correlated variable we can choose without affecting the multicollinearity would be vehicle weight (`cars$Weight`). Before digging too deep into this additonal variable on our model, we can build a pairwise scatterplot of the three variables to see how they interact.

```{r}
plot(cars[c(7, 9, 15)])
```

As we did with the original model, let's create a new variable which holds a model that accounts for this additional explanatory variable.

```{r}
model2 <- lm(CityMPG ~ Weight, data = complete_numerical_columns)
summary(model2)
```

As with the first explanatory variable, the p-value is extremely small (`2e-16`), which is a great sign that it's statistically significant. Our $R^2$ value is also decently high at `~0.68`, which equates to a reasonably strong correlation between the two variables `CityMPG` and `Weight`.

Let's visualize the calculations above.
```{r}
ggplot(data = complete_numerical_columns, aes(x = CityMPG, y = Weight), na.rm = TRUE) +
  geom_point() +
  geom_smooth(method = 'lm')
```

We can see almost the same data trend as with `FuelCap` in this graph. Following suite to the standard diagnostic procedures we performed for the first model, let's run through a few diagnostics to ensure our predicted explanatory variable works well.

```{r}
# Check for linearity
plot(model2$residuals ~ complete_numerical_columns$Weight)
abline(h = 0, lty = 3)
```

The data does display some trending, but it's spread considerably better than model1. Let's check for normality among distribution.

```{r}
# Check for normality (1/2)
hist(model2$residuals)
```

The data does appear to be somewhat centered around zero, with a slight right-tail. Using `qqnorm` we can check for normality in a different way.

```{r}
# Check for normality (2/2)
qqnorm(model2$residuals)
qqline(model2$residuals)
```

Once again from the above diagrams, we can conclude:

a) The data appears to meet linearity requirements, as it's scattered across the plane

b) The distribution is relatively normal meeting normality requirements

c) We appear to have equal variance, at least for the most part meeting the third requirement of equal variance.

##### Final Regression Model
Pulling all of this data together, let's build the final model with `FuelCap` and `Weight` as the explanatory variables and `CityMPG` as the response variable.

```{r}
model3 <- lm(CityMPG ~ FuelCap + Weight, data = complete_numerical_columns)
summary(model3)
```

With this additional explanatory variable included, we can see our $R^2$ value has actually jumped quite a decent amount to `~0.69` which is considered to be a moderately strong correlation. 

As a quick recap of the model diagnostics, we can run a residual error calculation to see the percentage of error in our model's calculations.

```{r}
print(sigma(model3) / mean(complete_numerical_columns$CityMPG))
```

We're sitting at a relatively low `0.12%` residual error percentage. Next in line for diagnostics, we can run the following four tools.

```{r}
# Analysis of Variance
anova(model3)

# Outliers (Bonferonni)
outlierTest(model3)

# QQ Plot
qqPlot(model3, main = "QQ Plot")

# Leverage Points in data
leveragePlots(model3)
```

Both the anova and bonferonni tests come up indicating that we have no significant issues with outliers in our dataset or analysis. Additionally, if we wanted to test the variance across multiple different models, we could run the anova test with each different model (I.E. `anova(model1, model2)`). However, because I built the model from the ground up, I don't need to throw in guesswork as to which model would be most appropriate for this task.

##### Summary
We do see a strong correlation between the `CityMPG`, `Weight`, and `FuelCap` variables. I am failing to reject the null hypothesis ($H_0$) due to this fact. See Recommendations below for a more in-depth summary.

### Analysis 5:[Multiple Regression 2 - One Quantitative/One Categorical Explanatory Variables]
#### Analyst:[Delmar Wilmont Randolph]

To begin, we need to import necessary packages.
```{r}
#import packages
library(tidyverse)
library (sjstats)
library (pwr)
install.packages("car")
library (car)
```
Import the Cars 2015 data set and remove rows with any missing values.
```{r}
# Load Data
cars <- read.csv("./Cars2015.csv")
cars
```
[H_0: There is a strong correlation between the high-price of a vehicle, it's fuel capacity, and city MPG. I propose that we will see a positively correlated linear relationship between the high-price, the fuel capacity, and city MPG of a vehicle.

H_A: There is an insignificant or non-existent correlation between a vehicle's high-price, the fuel capacity, and city MPG.]

### Response Variable
The chosen response variable for this analysis will be `High-Price`; Visualize the data behind this variable.
```{r}
ggplot(data = cars) +
  xlab("High-Price") +
  geom_boxplot(mapping = aes(x = HighPrice))
ggplot(data = cars) +
  xlab("High-Price") +
  geom_histogram(mapping = aes(x = HighPrice), binwidth = 0.75)
```
##### Explanatory Variable 1
Finding predictors of our response variable (`HighPrice`), requires a correlation matrix. We will find highly correlated variables and work from there.

```{r}
# Pull the complete columns from the data set
numerical_columns <- cars[complete.cases(cars), ]
  
# Pull the complete numerical columns from the data set
# Correlation matrix
complete_numerical_columns <- numerical_columns[, sapply(numerical_columns, is.numeric)]
print(round(cor(complete_numerical_columns), 2))

# Covariance matrix
print(round(cov(complete_numerical_columns), 2))
```
From the data above, we can see some correlations between our response variable and other quantitative / explanatory variables. We will begin with `HighPrice` and `FuelCap`. It is important to note that they are POSITIVELY correlated with each other. Specifically, they are correlated at `0.47`-- which can be expected -- as the price increases, the fuel capacity of the vehicle also increases - reasons for this could be a variety of factors (size, type, etc.)

Let's analyze this relationship with a simple linear model.

```{r}
model3 <- lm(HighPrice ~ FuelCap, data = complete_numerical_columns)
summary(model3)
```

We can see from the above calculations that both our regular p-value and anova f-statistic P-value is over 0.05, specifically around 0.25781, which suggests that changes in the predictor are not associated with changes in the response. Additionally the $R^2$ value of `~0.20` (20%) indicates that our variable does account for much of the variance.

Let's visualize the above correlations .

```{r}
ggplot(data = complete_numerical_columns, aes(x = HighPrice, y = FuelCap), na.rm = TRUE) +
  geom_point() +
  geom_smooth(method = 'lm')
```

Next let's run the standard residuals diagnostics on our first predictor variable.

```{r}
# Check for linearity
plot(model3$residuals ~ complete_numerical_columns$FuelCap)
abline(h = 0, lty = 3)

# Check for normality (1/2)
hist(model3$residuals)

# Check for normality (2/2)
qqnorm(model3$residuals)
qqline(model3$residuals)
```


From the data above, we can conclude:

a) The variable does not meet the linearity requirements; The response variable is not evenly scattered, and heavily condensed around the 10-20 range (Linearity)

b) Based off of the histogram, we can see the distribution is somewhat normal (Normality)

c) Similar to our linearity requirement, we do not have equal variance as required. (Equal variance)

##### Explanatory Variable 2
The next correlated variable we can choose would be the City MPG (`cars$CityMPG`). A visual aid would be beneficial in interpreting this variable.
```{r}
plot(cars[c(5, 7, 9)])
```

Create a new variable which holds a model that accounts for this explanatory variable.

```{r}
model2 <- lm(HighPrice ~ CityMPG, data = complete_numerical_columns)
summary(model2)
```

As with the first explanatory variable, the p-value is larger than 0.05, which shows that it is not statistically significant. Our $R^2$ value is also low at `~0.30`, which equates to a weak correlation between the two variables `HighPrice` and `CityMPG`.

Let's visualize the calculations above.
```{r}
ggplot(data = complete_numerical_columns, aes(x = HighPrice, y = CityMPG), na.rm = TRUE) +
  geom_point() +
  geom_smooth(method = 'lm')
```

We can see there is a NEGATIVE correlation - the higher the price, the less City MPG the vehicle has. Following the previous variable, we need to perform the standard diagnostic procedures we performed for the first model, let's run through a few diagnostics to ensure our predicted explanatory variable works.

```{r}
# Check for linearity
plot(model2$residuals ~ complete_numerical_columns$Weight)
abline(h = 0, lty = 3)
```

The data does not display any trending, but it's spread considerably better than model1. Let's check for normality among distribution.

```{r}
# Check for normality (1/2)
hist(model2$residuals)
```

The data does appear to be somewhat skewed-right, with a right-tail. Using `qqnorm` we can check for normality in a different way.

```{r}
# Check for normality (2/2)
qqnorm(model2$residuals)
qqline(model2$residuals)
```

Once again from the above diagrams, we can conclude:

a) The data appears to meet linearity requirements, as it's scattered across the plane

b) The distribution is not normal, not meeting requirements

c) We appear to have equal variance, at least for the most part meeting the third requirement of equal variance.

##### Final Regression Model
Pulling all of this data together, let's build the final model with `FuelCap` and `CityMPG` as the explanatory variables and `HighPrice` as the response variable.

```{r}
# Fit a multiple linear regression model for high-price as a function of the city MPG, and fuel tank Capacity

finalMod <- lm(HighPrice ~ CityMPG + FuelCap, data = complete_numerical_columns)
summary(finalMod)
```
With this additional explanatory variable included, we can see our $R^2$ value has remained below 30%, which is a relatively weak correlation.

As a quick recap of the model diagnostics, we can run a residual error calculation to see the percentage of error in our model's calculations.

```{r}
print(sigma(finalMod) / mean(complete_numerical_columns$HighPrice))
```

We're sitting at a relatively low `0.48%` residual error percentage. Next in line for diagnostics, we can run the following four tools.

```{r}
# Analysis of Variance
anova(finalMod)

# Outliers (Bonferonni)
outlierTest(finalMod)

# QQ Plot
qqPlot(finalMod, main = "QQ Plot")

# Leverage Points in data
leveragePlots(finalMod)
```

Both the anova and bonferonni test results indicate that we have no significant issues with outliers in our data set or analysis. Additionally, if we wanted to test the variance across multiple different models, we could run the anova test with each different model.

##### Summary
We do not see a significant correlation between the `HighPrice`, `CityMPG`, and `FuelCap` variables. I reject the null hypothesis due to this fact. See Recommendations below for a more in-depth summary.



## Recommendations
##### Analysis 1:
my estimate is the type of drive and the size of a car are dependent, I thought AWD cars' size will larger than FWD cars and RWD cars. However, analysis#1 proves my guess is wrong. Based on the sample data, we get there is no significant evidence to conclude that there is an association between the type of drive and the size of the car at the 95% confidence level. One limitation in my finding is the sample size is not representative enough for all cars in 2015.

##### Analysis 2: James Igwenagu
I thought that the different varity of cars manufacture or make would vary in the Fuel capacity of the different vehicles. Depending on the size of the cars i thought here would be a huge difference in fuelcap. From analysis 2 it shows that there was little to no difference. A limitation i found was that depending on the make of the vehicle the fuel cap was almost the same for all od them and we also have to out into account the brand names of the vehicle and the type of fuel needed. 

##### Analysis 3:

##### Analysis 4:
My goal in the analysis was to prove a relationship between the response variable City MPG and explanatory variables Dry Weight and Fuel Tank Capacity. The efforts to do so were successful, and there was found to be a negative correlation between the `CityMPG`:`FuelCap`. This relationship is nothing short of expected, as the fuel efficiency of a engine increases, one would expect that the need for a larger fuel tank decreases. Beyond fuel tank capacity, I also found that there was once again a negative linear correlation between `CityMPG`:`Weight`; As the `CityMPG` went up, the vehicle's weight went down. Need I repeat, this was not unexpected, heavy vehicles need larger engines, and larger engines mean more fuel consumption (Lower MPGs). I don't believe there are any limitations to the findings in this dataset, the only future limitation would be if eMPG (Electric simulated MPG) were included (I.E. Telas/Chevy Volt). This  addition would skew the data in many unfavorable ways. I would suspect that this analysis did not present any new findings to the reader, as it's all basic logic.

##### Analysis 5:

[my estimate is the higher price range of the vehicle would be affected by the City MPG, and Fuel capacity of the vehicle, and proving that the latter two variables are dependent on each other, I thought these two variables would drastically effect the price of the vehicle, however my analyses proved otherwise. Based on the sample data, we conclude there is no significant evidence to verify that there is an association between the City MPG, fuel capacity and price of the vehicle at the 95% confidence level. One limitation in my findings is the sample size is not representative enough for all cars in 2015. If one were to make their car purchase decision based on these variables, one would need to know that they do not correlate to the overall price of the vehicle.]

## Reflections
##### Alec Mattu:
I would say that the three unit projects significantly expanded my understanding of performing statistical analyses. Not only did it provide and opportunity to perform three independent analyses on the same dataset from different perspectives, it also provided the chance to view fellow student's ways of performing statistical functions and tasks. In summary, these past three projects allowed me to personally explore: Analysis on a single categorical variable, draw inferences on a single categorical proportion, and finally, multiple linear regression on two quantitative variables and one response variable. While I learned a plethora of things from unit projects, I would say that the most important is how to implement linear regression on multiple variables (categorical and numerical). Thankfully, I personally faced very few challenges when completing these unit projects--The only initial hiccup was setting up proper `.Rmd` file exporting to HTML/PDFs. 

##### Yiling Du:
Three Unit Projects help me to use the statistics knowledge I learned in class in a real data file. I learned how to use R language to analyze big data. Because of the Unit projects, I got more familiar to write R code. In addition, I learned we can get a lot of information through analyzing a data file, such as we made five different analyses for each unit project and we have different interesting findings for each analysis. I have not faced many challenges in completing the unit projects because the R demo videos cover most of the skills to use R.

[Add text section on your reflections about unit projects. Be sure to address each of the prompts on the instruction sheet.
a. What impact did the Unit Projects have on your understanding of statistics? b. What new knowledge did you gain about your statistical analyses? Briefly explain.c.  What did you learn by creating the unit projects? d.  What challenges did you face completing the unit projects? ]

##### Delmar Randolph:
I believe that these unit projects have immensely expanded my knowledge of statistics and working with R code. Specifically, from Unit 1 to the latest project, I felt that my skills working in statistics have only improved. Some challenges I faced were the fact that I have not taken a statistics course in a long while. Therefore, I had to re-familiarize myself with the concepts in order to perform adequately in this class. Thankfully, it provided the chance to view fellow student's ways of performing statistical functions and tasks, as well as their methods in thinking and approaching a problem. In summary, these past three projects allowed me to perform various modes of statistical computation and working with big data such as: Analysis on a single categorical variable, draw inferences on a single categorical proportion, and lastly, multiple linear regression on two quantitative variables and one response variable.

##### James Igwenagu
These projects have shown me yet another way to and platform to use to Calculate statistical data using a data set. I was always use ti using python which is a little harder. R was more Straight forward and easier to understand and use.  I learned a lot about Statistical data Calculations and how to clean and refine it. It was a great experience. I dont know if i will be able to implement using this in the future but it good to have a grounded understanding of it. 
